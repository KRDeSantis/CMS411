


# From YouTube tutorial: https://www.youtube.com/watch?v=XcZGKAF5zxg
# Packages
import docx2txt

import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lemmatizer import Lemmatizer
from spacy.lookups import Lookups
from collections import defaultdict



# change the text to whatever document you want to process.
document1 = docx2txt.process("test1.docx")  #### File location
NUM_SENT=5
stopwords = list(STOP_WORDS)

len(stopwords)
nlp = spacy.load('en_core_web_md')
docx = nlp(document1)
sentences = [sent.string.strip() for sent in docx.sents]

# Creates a list of non-stopwords and adds a counter for
# how often they occur in the document
word_frequencies = defaultdict(int)
words = {}
lookups=Lookups()
lemmatizer=Lemmatizer(lookups)

for word in docx:
    if lemmatizer.lookup(word.text) not in stopwords:
            word_frequencies[lemmatizer.lookup(word.text),0] += 1



sentenceList = []
topTotals = []


for sent in docx.sents:
    sentenceTotal = 0
    for word in sent:
        sentenceTotal += word_frequencies[lemmatizer.lookup(word.text)]
    if(len ( topTotals) < NUM_SENT):
        sentenceList.append(sent)
        topTotals.append(sentenceTotal)
    else:
        i = 0
        while(i<NUM_SENT):
            if int(sentenceTotal) > int(topTotals[i]):
                swapS = sentenceList[i]
                swapN = topTotals[i]
                sentenceList[i] = sent
                topTotals[i] = sentenceTotal
                sentenceList[i+1] = swapS
                topTotals[i+1] = swapS
                if(len(sentenceList)==NUM_SENT+1):
                    del sentenceList[NUM_SENT+1]
                if(len(topTotals)==NUM_SENT+1):
                    del topTotals[NUM_SENT+1]
            i=i+1




for sent in sentenceList:
    print(str(sent))














